"""Maya performs inference based on the user's query and context retrieved by Stella.

Maya is part of the `researchers` package.
"""

import os
import sys
from langchain.prompts import PromptTemplate

# add parent directory to Python path
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
sys.path.append(os.path.dirname(SCRIPT_DIR))

# load researcher functions and utilities
from researchers.utilities import configure_llm
from researchers.stella import retrieve_context
from researchers.orion import decompose_query_into_search_phrases


def initialize_prompt_for_response_generation(context, query):
    # setup prompt template
    prompt_template = PromptTemplate.from_template(
        """You are Maya, a friendly and helpful AI chatbot in a team of AI chatbots who are assisting a user in medical research.

Your role is answering queries about medical research. You have been asked the following query:

{query}

You must answer based on the following context:

{context}

---

Do not attempt to answer from your own knowledge. Provide citations in your answer using the information
provided in the above context.

Your response:"""
    )

    # initialize prompt with inputs
    prompt = prompt_template.format(
        query=query,
        context=context,
    )

    print(prompt)
    return prompt


def generate_response(context, query):
    """Maya's main function.

    This function uses the context retrieved by Stella and the user's query
    to generate a meaningful, knowledge-enriched response.

    Args:
        context (str): the context required to respond to the user's query
        query (str): the user's query

    Returns:
        sre: response generated by Maya using an LLM
    """

    print(
        f"INFO: Maya's main function ({generate_response.__name__}) has been invoked."
    )

    # set up LLM
    llm = configure_llm()

    # initialize prompt
    prompt = initialize_prompt_for_response_generation(
        context=context,
        query=(query),
    )

    # get output from LLM
    response = llm.invoke(prompt)

    return response


def main():
    """Runs demo of Maya in action."""

    # initialize demo query for Maya
    query = "What studies have been done regarding transmucosal implant placement?"

    # decompose query into search phrases
    search_phrases = decompose_query_into_search_phrases(query)

    # get context from Stella
    context = retrieve_context(
        search_phrases,
        query,
    )

    # get output from LLM
    response = generate_response(
        context,
        query,
    )

    print(f"Output: {response}")


if __name__ == "__main__":
    main()
